{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff027d21-6db8-4c1b-8cb2-8aff8c1923fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 35_risk_eval_build — Build per-loan risk outputs from features + rules\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Parameters\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "dbutils.widgets.text(\"CATALOG\", \"reporting_factory_risk_profile\")\n",
    "dbutils.widgets.text(\"RUN_TAG\", \"rules@current\")   # any free-form string for traceability\n",
    "catalog = dbutils.widgets.get(\"CATALOG\")\n",
    "run_tag = dbutils.widgets.get(\"RUN_TAG\")\n",
    "\n",
    "spark.sql(f\"USE CATALOG {catalog}\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS control\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS gold\")\n",
    "spark.sql(\"USE SCHEMA gold\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2) Guardrails: required inputs exist & non-empty\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def fail(msg):\n",
    "    print(f\"❌ {msg}\")\n",
    "    raise Exception(msg)\n",
    "\n",
    "# gold.features must exist and have rows\n",
    "if not spark._jsparkSession.catalog().tableExists(f\"{catalog}.gold.features\"):\n",
    "    fail(f\"Missing table: {catalog}.gold.features\")\n",
    "\n",
    "n_feat = spark.sql(\"SELECT COUNT(*) c FROM gold.features\").first().c\n",
    "if n_feat == 0:\n",
    "    fail(\"gold.features is empty.\")\n",
    "\n",
    "# rules table may exist; if not, we’ll default\n",
    "rules_exists = spark._jsparkSession.catalog().tableExists(f\"{catalog}.control.risk_rules\")\n",
    "if not rules_exists:\n",
    "    print(\"⚠️ control.risk_rules not found — will default all loans to Medium / 0 points.\")\n",
    "else:\n",
    "    # Create a convenience view of today's enabled rules\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW tmp_enabled_rules AS\n",
    "    SELECT rule_id, name, segment, condition_sql, impact_column, impact_value, priority\n",
    "    FROM control.risk_rules\n",
    "    WHERE enabled = TRUE\n",
    "      AND current_date BETWEEN effective_from AND COALESCE(effective_to, DATE '2999-12-31')\n",
    "    ORDER BY priority\n",
    "    \"\"\")\n",
    "    n_rules = spark.sql(\"SELECT COUNT(*) c FROM tmp_enabled_rules\").first().c\n",
    "    print(f\"Enabled rules (effective today): {n_rules}\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 3) Build a unified matches SQL (union of all rules against gold.features)\n",
    "#    Each rule’s condition_sql is written against alias 'f' (as designed earlier).\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "from datetime import datetime\n",
    "run_id = f\"RE_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\"  # Risk Eval run id\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "if rules_exists:\n",
    "    # Collect rule rows and generate UNION ALL SQL\n",
    "    rules_pdf = spark.sql(\"SELECT * FROM tmp_enabled_rules ORDER BY priority\").toPandas()\n",
    "    union_parts = []\n",
    "    for _, r in rules_pdf.iterrows():\n",
    "        rid = r[\"rule_id\"]\n",
    "        impact_col = r[\"impact_column\"]\n",
    "        impact_val = r[\"impact_value\"]\n",
    "        prio = int(r[\"priority\"]) if r[\"priority\"] is not None else 0\n",
    "        cond = r[\"condition_sql\"] or \"1=0\"  # safety\n",
    "\n",
    "        union_parts.append(f\"\"\"\n",
    "            SELECT f.loan_id,\n",
    "                   '{rid}'  AS rule_id,\n",
    "                   '{impact_col}' AS impact_column,\n",
    "                   '{impact_val}' AS impact_value,\n",
    "                   {prio}   AS priority\n",
    "            FROM {catalog}.gold.features f\n",
    "            WHERE ({cond})\n",
    "        \"\"\".strip())\n",
    "\n",
    "    matches_sql = \" UNION ALL \".join(union_parts) if union_parts else \"SELECT NULL AS loan_id, NULL AS rule_id, NULL AS impact_column, NULL AS impact_value, 0 AS priority WHERE 1=0\"\n",
    "else:\n",
    "    # No rules — empty matches\n",
    "    matches_sql = \"SELECT NULL AS loan_id, NULL AS rule_id, NULL AS impact_column, NULL AS impact_value, 0 AS priority WHERE 1=0\"\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 4) Resolve risk_band and risk_points per loan and publish gold.risk_eval\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "resolved_sql = f\"\"\"\n",
    "CREATE OR REPLACE TABLE gold.risk_eval\n",
    "USING DELTA\n",
    "AS\n",
    "WITH matches AS (\n",
    "  {matches_sql}\n",
    "),\n",
    "band AS (\n",
    "  SELECT loan_id, impact_value AS risk_band\n",
    "  FROM (\n",
    "    SELECT loan_id, impact_value, priority,\n",
    "           ROW_NUMBER() OVER (PARTITION BY loan_id ORDER BY priority DESC) rn\n",
    "    FROM matches\n",
    "    WHERE lower(impact_column)='risk_band'\n",
    "  ) x\n",
    "  WHERE rn=1\n",
    "),\n",
    "points AS (\n",
    "  SELECT loan_id,\n",
    "         COALESCE(SUM(CAST(regexp_extract(impact_value,'[-+]?\\\\d+',0) AS INT)),0) AS risk_points\n",
    "  FROM matches\n",
    "  WHERE lower(impact_column)='risk_points'\n",
    "  GROUP BY loan_id\n",
    ")\n",
    "SELECT\n",
    "  f.loan_id,\n",
    "  f.borrower_id,\n",
    "  f.dti,\n",
    "  f.fico_score,\n",
    "  f.utilization,\n",
    "  f.grade,\n",
    "  f.loan_amount,\n",
    "  f.interest_rate,\n",
    "  f.term_months,\n",
    "  f.issue_date,\n",
    "  COALESCE(b.risk_band, 'Medium') AS risk_band,\n",
    "  COALESCE(p.risk_points, 0)      AS risk_points,\n",
    "  '{run_tag}'                      AS rules_version,\n",
    "  current_timestamp()              AS evaluated_at\n",
    "FROM gold.features f\n",
    "LEFT JOIN band   b ON f.loan_id = b.loan_id\n",
    "LEFT JOIN points p ON f.loan_id = p.loan_id\n",
    "\"\"\"\n",
    "spark.sql(resolved_sql)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 5) Lightweight audit log\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS control.rule_runs (\n",
    "  risk_eval_run_id STRING,\n",
    "  rules_version STRING,\n",
    "  rule_count INT,\n",
    "  features_count BIGINT,\n",
    "  produced_at TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "rule_count = spark.sql(\"SELECT COUNT(*) c FROM tmp_enabled_rules\").first().c if rules_exists else 0\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO control.rule_runs\n",
    "VALUES ('{run_id}', '{run_tag}', {rule_count}, {n_feat}, current_timestamp())\n",
    "\"\"\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 6) Quick stats & finish\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "out = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  COUNT(*) AS loans_total,\n",
    "  SUM(CASE WHEN risk_band='High' OR risk_points>=20 THEN 1 ELSE 0 END) AS high_risk_loans,\n",
    "  ROUND(AVG(dti),1)  AS avg_dti,\n",
    "  ROUND(AVG(fico_score),0) AS avg_fico\n",
    "FROM gold.risk_eval\n",
    "\"\"\").first()\n",
    "\n",
    "print(f\"✅ gold.risk_eval built (run_id={run_id}, rules_version={run_tag})\")\n",
    "print(f\"   loans_total={out.loans_total}, high_risk_loans={out.high_risk_loans}, avg_dti={out.avg_dti}, avg_fico={out.avg_fico}\")\n",
    "dbutils.jobs.taskValues.set(key=\"risk_eval_run_id\", value=run_id)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "35_risk_eval_build",
   "widgets": {
    "CATALOG": {
     "currentValue": "reporting_factory_risk_profile",
     "nuid": "00dc0385-a140-4382-ab61-97ccd38005c5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "reporting_factory_risk_profile",
      "label": null,
      "name": "CATALOG",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "reporting_factory_risk_profile",
      "label": null,
      "name": "CATALOG",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "RUN_TAG": {
     "currentValue": "rules@current",
     "nuid": "636b2ad3-56f9-4cc4-ac9d-6ef3bba17ce8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "rules@current",
      "label": null,
      "name": "RUN_TAG",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "rules@current",
      "label": null,
      "name": "RUN_TAG",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
