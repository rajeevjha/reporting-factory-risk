{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40e3258e-f790-4ec5-9f2d-339fa8c0f4dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC ## üßπ Reporting Factory ‚Äì Cleanup Temporary Tables\n",
    "# MAGIC This notebook truncates intermediate tables to save cost and keep your workspace lean.  \n",
    "# MAGIC It **keeps approved & control data** (risk_rules, risk_eval, report_runs, report_facts).\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Setup\n",
    "dbutils.widgets.text(\"CATALOG\", \"reporting_factory_risk_profile\")\n",
    "catalog = dbutils.widgets.get(\"CATALOG\")\n",
    "\n",
    "spark.sql(f\"USE CATALOG {catalog}\")\n",
    "print(f\"Using catalog: {catalog}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,List of tables to truncate\n",
    "cleanup_targets = [\n",
    "    \"bronze.lending_raw\",\n",
    "    \"silver.loans\",\n",
    "    \"silver.borrowers\",\n",
    "    \"gold.features\"\n",
    "]\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Truncate the tables (if they exist)\n",
    "for tbl in cleanup_targets:\n",
    "    try:\n",
    "        print(f\"‚è≥ Truncating {tbl} ...\")\n",
    "        spark.sql(f\"TRUNCATE TABLE IF EXISTS {tbl}\")\n",
    "        print(f\"‚úÖ Truncated {tbl}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Skipped {tbl} ‚Äî {e}\")\n",
    "\n",
    "print(\"‚úÖ Cleanup complete ‚Äî temporary tables cleared.\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Optional: Drop staging or temp tables\n",
    "# If your pipeline creates temporary / staging tables, list them here\n",
    "staging_tables = [\n",
    "    # Example:\n",
    "    # \"gold.temp_risk_eval\",\n",
    "    # \"silver.tmp_borrower_snapshot\"\n",
    "]\n",
    "\n",
    "for tbl in staging_tables:\n",
    "    try:\n",
    "        print(f\"üóë Dropping {tbl} ...\")\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {tbl}\")\n",
    "        print(f\"‚úÖ Dropped {tbl}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not drop {tbl}: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Optional: Compact small Delta files (save cost)\n",
    "optimize_targets = [\n",
    "    \"gold.risk_eval\",\n",
    "    \"gold.report_facts\",\n",
    "    \"gold.report_runs\"\n",
    "]\n",
    "\n",
    "for tbl in optimize_targets:\n",
    "    try:\n",
    "        print(f\"‚ôªÔ∏è Optimizing {tbl} ...\")\n",
    "        spark.sql(f\"OPTIMIZE {tbl}\")\n",
    "        print(f\"‚úÖ Optimized {tbl}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not optimize {tbl}: {e}\")\n",
    "\n",
    "print(\"üèÅ Cleanup and compaction complete.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "90_Cleanup_Temporary_Tables",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
