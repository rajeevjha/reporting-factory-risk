{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Streamlit Rule Builder \u2013 Enhanced\nFull CRUD on rules (add/edit/delete), preview impact, build draft, and approve."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import streamlit as st\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import Row\n\nst.set_page_config(page_title=\"Risk Rules\", layout=\"wide\")\nst.title(\"Risk Rules \u2013 Builder & Approval (Enhanced)\")\n\n# --------- Config ---------\nDEFAULT_CATALOG = \"<CATALOG_NAME>\"\nALLOWED_IMPACT_COLUMNS = {\"risk_band\",\"risk_points\"}\nALLOWED_SEGMENTS = {\"retail\",\"corporate\",\"all\"}\n\n# --------- Inputs ---------\ncatalog = st.text_input(\"Catalog (Unity Catalog)\", value=DEFAULT_CATALOG, help=\"e.g., reporting_factory\")\n_ = spark.sql(f\"USE CATALOG {catalog}\")\nspark.sql(\"USE SCHEMA control\")\n\n# Ensure table exists\nspark.sql(\"\"\"\nCREATE TABLE IF NOT EXISTS control.risk_rules\n(rule_id STRING, name STRING, segment STRING, condition_sql STRING,\n impact_column STRING, impact_value STRING, priority INT, enabled BOOLEAN,\n effective_from DATE, effective_to DATE, owner STRING, notes STRING)\nUSING DELTA\n\"\"\")\n\nst.caption(\"Editing below writes directly to control.risk_rules in Unity Catalog (auditable).\")\n\n# --------- Load rules ---------\nrules_sdf = spark.table(\"control.risk_rules\")\nrules_pdf = rules_sdf.orderBy(\"priority\",\"rule_id\").toPandas()\n\n# Editable columns (safe set)\neditable_cols = [\"name\",\"segment\",\"condition_sql\",\"impact_column\",\"impact_value\",\"priority\",\"enabled\",\"notes\"]\n\nst.subheader(\"Edit Rules Inline\")\nst.write(\"Tip: Use `risk_band` values like Low/Medium/High, and `risk_points` like +10 / -5.\")\nedited = st.data_editor(\n    rules_pdf[[\"rule_id\"] + editable_cols],\n    use_container_width=True,\n    num_rows=\"dynamic\",\n    disabled=[\"rule_id\"],  # editing rule_id is not supported in-place\n    key=\"rules_editor\"\n)\n\n# Detect changes by comparing to original\nchanged_rows = []\nif st.button(\"Save Changes\"):\n    before = rules_pdf.set_index(\"rule_id\")\n    after = edited.set_index(\"rule_id\")\n    to_update = []\n\n    for rid in after.index:\n        if rid not in before.index:\n            continue\n        row_before = before.loc[rid]\n        row_after = after.loc[rid]\n        if not row_before.equals(row_after):\n            # basic validations\n            seg = row_after[\"segment\"]\n            col = row_after[\"impact_column\"]\n            imp = str(row_after[\"impact_value\"])\n            pri = int(row_after[\"priority\"])\n            en  = bool(row_after[\"enabled\"])\n\n            if seg not in ALLOWED_SEGMENTS:\n                st.error(f\"Rule {rid}: invalid segment '{seg}'. Allowed: {ALLOWED_SEGMENTS}\")\n                st.stop()\n            if col not in ALLOWED_IMPACT_COLUMNS:\n                st.error(f\"Rule {rid}: invalid impact_column '{col}'. Allowed: {ALLOWED_IMPACT_COLUMNS}\")\n                st.stop()\n            if col == \"risk_points\":\n                import re\n                if not re.fullmatch(r\"[+\\-]?\\d+\", imp):\n                    st.error(f\"Rule {rid}: impact_value must be integer-like for risk_points (e.g., +15). Got: {imp}\")\n                    st.stop()\n            to_update.append((rid, row_after.to_dict()))\n\n    if to_update:\n        for rid, data in to_update:\n            set_exprs = []\n            for c in editable_cols:\n                val = data[c]\n                if isinstance(val, bool):\n                    set_exprs.append(f\"{c} = {str(val).lower()}\")\n                elif isinstance(val, (int, float)):\n                    set_exprs.append(f\"{c} = {val}\")\n                else:\n                    # escape single quotes\n                    sval = str(val).replace(\"'\", \"''\")\n                    set_exprs.append(f\"{c} = '{sval}'\")\n            spark.sql(f\"\"\"UPDATE control.risk_rules SET {', '.join(set_exprs)} WHERE rule_id = '{rid}'\"\"\")\n        st.success(f\"Updated {len(to_update)} rule(s). Refresh to see changes.\")\n    else:\n        st.info(\"No changes detected.\")\n\n# --------- Add new rule ---------\nst.subheader(\"Add New Rule\")\nwith st.form(\"add_rule\"):\n    c1, c2, c3 = st.columns(3)\n    with c1:\n        new_id = st.text_input(\"rule_id\", placeholder=\"R5\", max_chars=50)\n        new_name = st.text_input(\"name\", placeholder=\"fico_very_low\")\n        new_segment = st.selectbox(\"segment\", sorted(ALLOWED_SEGMENTS))\n        new_priority = st.number_input(\"priority\", value=50, min_value=1, step=1)\n        new_enabled = st.checkbox(\"enabled\", value=True)\n    with c2:\n        new_impact_column = st.selectbox(\"impact_column\", sorted(ALLOWED_IMPACT_COLUMNS))\n        new_impact_value = st.text_input(\"impact_value\", placeholder=\"High or +15\")\n        new_owner = st.text_input(\"owner\", value=\"risk_ops\")\n    with c3:\n        new_condition = st.text_area(\"condition_sql\", placeholder=\"f.fico_score < 580 AND f.dti > 35\", height=100)\n        new_notes = st.text_input(\"notes\", placeholder=\"Explain the rule intent\")\n    submitted = st.form_submit_button(\"Insert Rule\")\n\nif submitted:\n    # validations\n    if not new_id or not new_name or not new_condition:\n        st.error(\"rule_id, name and condition_sql are required.\")\n        st.stop()\n    if new_segment not in ALLOWED_SEGMENTS:\n        st.error(f\"segment must be one of {ALLOWED_SEGMENTS}\")\n        st.stop()\n    if new_impact_column not in ALLOWED_IMPACT_COLUMNS:\n        st.error(f\"impact_column must be one of {ALLOWED_IMPACT_COLUMNS}\")\n        st.stop()\n    if new_impact_column == \"risk_points\":\n        import re\n        if not re.fullmatch(r\"[+\\-]?\\d+\", new_impact_value or \"\"):\n            st.error(\"impact_value must be integer-like for risk_points (e.g., +20)\")\n            st.stop()\n    # escape strings\n    def esc(s): return str(s).replace(\"'\", \"''\")\n    spark.sql(f\"\"\"\n        INSERT INTO control.risk_rules\n        (rule_id, name, segment, condition_sql, impact_column, impact_value, priority, enabled, \n         effective_from, effective_to, owner, notes)\n        VALUES\n        ('{esc(new_id)}','{esc(new_name)}','{new_segment}','{esc(new_condition)}','{new_impact_column}',\n         '{esc(new_impact_value)}',{int(new_priority)},{str(bool(new_enabled)).lower()}, current_date, NULL,\n         '{esc(new_owner)}','{esc(new_notes)}')\n    \"\"\")\n    st.success(f\"Inserted rule {new_id}.\")\n\n# --------- Delete rule ---------\nst.subheader(\"Delete Rule\")\nrid_del = st.text_input(\"rule_id to delete\", placeholder=\"R3\")\nif st.button(\"Delete Rule\"):\n    if not rid_del:\n        st.warning(\"Enter a rule_id to delete.\")\n    else:\n        spark.sql(f\"DELETE FROM control.risk_rules WHERE rule_id = '{rid_del.replace(\"'\",\"''\")}'\")\n        st.success(f\"Deleted rule {rid_del} (if it existed).\")\n\nst.markdown(\"---\")\nst.subheader(\"Preview Impact (1% sample)\")\nif st.button(\"Run Preview on Sample\"):\n    spark.sql(\"USE SCHEMA gold\")\n    f = spark.table(\"gold.features\").sample(0.01, seed=42).alias(\"f\")\n    r = spark.table(\"control.risk_rules\").where(\n        \"enabled = true AND current_date BETWEEN effective_from AND coalesce(effective_to, date'2999-12-31')\"\n    )\n    matches = None\n    for row in r.collect():\n        pred = row[\"condition_sql\"]\n        part = f.selectExpr(\n            \"loan_id\",\n            f\"'{row['rule_id']}' as rule_id\",\n            f\"'{row['impact_column']}' as impact_column\",\n            f\"'{row['impact_value']}' as impact_value\",\n            f\"{row['priority']} as priority\"\n        ).where(pred)\n        matches = part if matches is None else matches.unionByName(part)\n    if matches is None:\n        preview = f.withColumn(\"matched_rules\", F.array())\\\n                   .withColumn(\"risk_band\", F.lit(\"Medium\"))\\\n                   .withColumn(\"risk_points\", F.lit(0))\n    else:\n        from pyspark.sql.window import Window\n        w = Window.partitionBy(\"loan_id\",\"impact_column\").orderBy(F.desc(\"priority\"))\n        top = matches.withColumn(\"rn\", F.row_number().over(w)).where(\"rn=1\")\n        preview = (f.join(top, on=\"loan_id\", how=\"left\")\n                     .groupBy(f.columns)\n                     .agg(F.collect_list(\"rule_id\").alias(\"matched_rules\"),\n                          F.max(F.when(F.col(\"impact_column\")==\"risk_band\", F.col(\"impact_value\"))).alias(\"risk_band\"),\n                          F.sum(F.when(F.col(\"impact_column\")==\"risk_points\",\n                                       F.regexp_extract(F.col(\"impact_value\"), \"[-+]?\\d+\", 0).cast(\"int\")).otherwise(0)).alias(\"risk_points\")))\\\n                     .fillna({\"risk_band\":\"Medium\",\"risk_points\":0,\"matched_rules\":[]})\n    c1, c2, c3 = st.columns(3)\n    c1.metric(\"Loans (sample)\", int(preview.count()))\n    c2.metric(\"Avg DTI\", float(preview.select(F.avg(\"dti\")).first()[0] or 0))\n    c3.metric(\"Avg FICO\", float(preview.select(F.avg(\"fico_score\")).first()[0] or 0))\n    st.write(\"High risk count (sample):\", preview.select(F.sum((F.col(\"risk_band\")==\"High\").cast(\"int\")).alias(\"high_risk_count\")).toPandas())\n    st.success(\"Preview complete.\")\n\nst.markdown(\"---\")\nst.subheader(\"Generate Draft Report\")\nif st.button(\"Build Draft Now\"):\n    spark.sql(\"USE SCHEMA gold\")\n    from datetime import datetime\n    run_id = f\"RR_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\"\n    spark.sql(f\"INSERT INTO gold.report_runs VALUES ('{run_id}','Risk Report','DRAFT','rules@current',current_timestamp(),NULL,NULL,NULL)\")\n    spark.sql(\"\"\"\n        CREATE OR REPLACE TEMP VIEW _kpi AS\n        SELECT 'loans_total' as metric, 'all' as dimension, count(*)*1.0 as value FROM gold.risk_eval\n        UNION ALL\n        SELECT 'high_risk_count','all', sum(CASE WHEN risk_band='High' OR risk_points>=20 THEN 1 ELSE 0 END) FROM gold.risk_eval\n        UNION ALL\n        SELECT 'avg_dti','all', avg(dti) FROM gold.risk_eval\n        UNION ALL\n        SELECT 'avg_fico','all', avg(fico_score) FROM gold.risk_eval\n    \"\"\")\n    spark.sql(f\"DELETE FROM gold.report_facts WHERE report_run_id = '{run_id}'\")\n    spark.sql(f\"INSERT INTO gold.report_facts SELECT '{run_id}', metric, dimension, value FROM _kpi\")\n    st.success(f\"Draft report created: {run_id}\")\n\nst.subheader(\"Approve Latest Draft\")\nif st.button(\"Approve Most Recent Draft\"):\n    latest = spark.sql(\"SELECT report_run_id FROM gold.report_runs WHERE status='DRAFT' ORDER BY started_at DESC LIMIT 1\").collect()\n    if latest:\n        rid = latest[0][\"report_run_id\"]\n        spark.sql(f\"\"\"\n            UPDATE gold.report_runs\n            SET status='APPROVED', approved_by=current_user(), approved_at=current_timestamp()\n            WHERE report_run_id='{rid}' AND status='DRAFT'\n        \"\"\")\n        spark.sql(\"\"\"\n            CREATE OR REPLACE VIEW gold.report_facts_approved_latest AS\n            SELECT rf.*\n            FROM gold.report_facts rf\n            JOIN (\n              SELECT report_run_id FROM gold.report_runs WHERE status='APPROVED' ORDER BY approved_at DESC LIMIT 1\n            ) latest USING (report_run_id);\n        \"\"\")\n        st.success(f\"Approved run: {rid}\")\n    else:\n        st.warning(\"No draft runs found.\")\n"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}