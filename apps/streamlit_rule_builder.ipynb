{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Streamlit Rule Builder\nInteractive editor + preview + approve."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import streamlit as st\nfrom pyspark.sql import functions as F\n\nst.title(\"Risk Rules \u2013 Builder & Approval\")\n\n# Inputs\ncatalog = st.text_input(\"Catalog\", value=\"<CATALOG_NAME>\")\nst.caption(\"Set at Job runtime or here for ad-hoc runs.\")\n_ = spark.sql(f\"USE CATALOG {catalog}\")\nspark.sql(\"USE SCHEMA control\")\n\n# Load rules\nrules = spark.table(\"control.risk_rules\")\nst.subheader(\"Rules\")\nst.dataframe(rules.toPandas())\n\n# Simple toggle UI (edit enable/priority inline)\nwith st.expander(\"Enable / Disable rules\"):\n    rule_ids = [r[\"rule_id\"] for r in rules.select(\"rule_id\").collect()]\n    selected = st.multiselect(\"Enable only these rules (others disabled):\", rule_ids, rule_ids)\n    if st.button(\"Apply enable/disable\"):\n        spark.sql(\"UPDATE control.risk_rules SET enabled = false\")\n        if selected:\n            spark.sql(f\"UPDATE control.risk_rules SET enabled = true WHERE rule_id IN ({','.join([f\"'{x}'\" for x in selected])})\")\n        st.success(\"Rule enablement updated.\")\n\nst.subheader(\"Preview impact (1% sample)\")\nif st.button(\"Run Preview\"):\n    spark.sql(\"USE SCHEMA gold\")\n    f = spark.table(\"gold.features\").sample(0.01, seed=42).alias(\"f\")\n    r = spark.table(\"control.risk_rules\").where(\"enabled = true AND current_date BETWEEN effective_from AND coalesce(effective_to, date'2999-12-31')\")\n    matches = None\n    for row in r.collect():\n        pred = row[\"condition_sql\"]\n        part = f.selectExpr(\n            \"loan_id\",\n            f\"'{row['rule_id']}' as rule_id\",\n            f\"'{row['impact_column']}' as impact_column\",\n            f\"'{row['impact_value']}' as impact_value\",\n            f\"{row['priority']} as priority\"\n        ).where(pred)\n        matches = part if matches is None else matches.unionByName(part)\n    if matches is None:\n        preview = f.withColumn(\"matched_rules\", F.array())\\\n                   .withColumn(\"risk_band\", F.lit(\"Medium\"))\\\n                   .withColumn(\"risk_points\", F.lit(0))\n    else:\n        from pyspark.sql.window import Window\n        w = Window.partitionBy(\"loan_id\",\"impact_column\").orderBy(F.desc(\"priority\"))\n        top = matches.withColumn(\"rn\", F.row_number().over(w)).where(\"rn=1\")\n        preview = (f.join(top, on=\"loan_id\", how=\"left\")\n                     .groupBy(f.columns)\n                     .agg(F.collect_list(\"rule_id\").alias(\"matched_rules\"),\n                          F.max(F.when(F.col(\"impact_column\")==\"risk_band\", F.col(\"impact_value\"))).alias(\"risk_band\"),\n                          F.sum(F.when(F.col(\"impact_column\")==\"risk_points\",\n                                       F.regexp_extract(F.col(\"impact_value\"), \"[-+]?\\d+\", 0).cast(\"int\")).otherwise(0)).alias(\"risk_points\")))\\\n                     .fillna({\"risk_band\":\"Medium\",\"risk_points\":0,\"matched_rules\":[]})\n    st.write(\"Sample KPIs:\")\n    st.write(preview.select(F.count(\"*\").alias(\"loans_total\")).toPandas())\n    st.write(preview.select(F.avg(\"dti\").alias(\"avg_dti\"), F.avg(\"fico_score\").alias(\"avg_fico\")).toPandas())\n    st.write(preview.select(F.sum((F.col(\"risk_band\")==\"High\").cast(\"int\")).alias(\"high_risk_count\")).toPandas())\n    st.success(\"Preview complete.\")\n\nst.subheader(\"Generate Draft Report\")\nif st.button(\"Build Draft Now\"):\n    spark.sql(\"USE SCHEMA gold\")\n    from datetime import datetime\n    run_id = f\"RR_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\"\n    spark.sql(f\"INSERT INTO gold.report_runs VALUES ('{run_id}','Risk Report','DRAFT','rules@current',current_timestamp(),NULL,NULL,NULL)\")\n    spark.sql(\"\"\"\n        CREATE OR REPLACE TEMP VIEW _kpi AS\n        SELECT 'loans_total' as metric, 'all' as dimension, count(*)*1.0 as value FROM gold.risk_eval\n        UNION ALL\n        SELECT 'high_risk_count','all', sum(CASE WHEN risk_band='High' OR risk_points>=20 THEN 1 ELSE 0 END) FROM gold.risk_eval\n        UNION ALL\n        SELECT 'avg_dti','all', avg(dti) FROM gold.risk_eval\n        UNION ALL\n        SELECT 'avg_fico','all', avg(fico_score) FROM gold.risk_eval\n    \"\"\")\n    spark.sql(f\"DELETE FROM gold.report_facts WHERE report_run_id = '{run_id}'\")\n    spark.sql(f\"INSERT INTO gold.report_facts SELECT '{run_id}', metric, dimension, value FROM _kpi\")\n    st.success(f\"Draft report created: {run_id}\")\n\nst.subheader(\"Approve Latest Draft\")\nif st.button(\"Approve Most Recent Draft\"):\n    latest = spark.sql(\"SELECT report_run_id FROM gold.report_runs WHERE status='DRAFT' ORDER BY started_at DESC LIMIT 1\").collect()\n    if latest:\n        rid = latest[0][\"report_run_id\"]\n        spark.sql(f\"\"\"\n            UPDATE gold.report_runs\n            SET status='APPROVED', approved_by=current_user(), approved_at=current_timestamp()\n            WHERE report_run_id='{rid}' AND status='DRAFT'\n        \"\"\")\n        spark.sql(\"\"\"\n            CREATE OR REPLACE VIEW gold.report_facts_approved_latest AS\n            SELECT rf.*\n            FROM gold.report_facts rf\n            JOIN (\n              SELECT report_run_id FROM gold.report_runs WHERE status='APPROVED' ORDER BY approved_at DESC LIMIT 1\n            ) latest USING (report_run_id);\n        \"\"\")\n        st.success(f\"Approved run: {rid}\")\n    else:\n        st.warning(\"No draft runs found.\")\n"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}